# 算子前向与反向传播原理详解

本文档详细说明 OriginDL 框架中各个算子的前向传播与反向传播数学原理，并通过具体例子展示计算过程，帮助快速理解算子实现。

## 目录

- [一、数学运算算子](#一数学运算算子)
- [二、激活函数算子](#二激活函数算子)
- [三、卷积运算算子](#三卷积运算算子)
- [四、池化运算算子](#四池化运算算子)
- [五、形状变换算子](#五形状变换算子)
- [六、神经网络层算子](#六神经网络层算子)
- [七、归一化算子](#七归一化算子)
- [八、损失函数算子](#八损失函数算子)
- [九、总结](#九总结)

---

## 一、数学运算算子

### 1.1 加法算子（Add）

#### 1.1.1 前向传播原理

加法算子的前向传播执行元素级加法运算：`y = x0 + x1`

**数学描述：**
- 对于相同位置的元素，输出等于两个输入对应元素的和：`y[i] = x0[i] + x1[i]`
- 当两个输入张量的形状不同时，支持广播机制：较小的张量会在缺失维度上扩展，然后进行元素级加法
- 广播规则：从最右边的维度开始，如果维度大小相同或其中一个为 1，则可以广播

#### 1.1.2 反向传播原理

对于加法运算 `y = x0 + x1`，根据链式法则和偏导数：
- `∂y/∂x0 = 1`，`∂y/∂x1 = 1`

因此，如果输出梯度为 `gy`，则：
- `gx0 = gy × ∂y/∂x0 = gy × 1 = gy`
- `gx1 = gy × ∂y/∂x1 = gy × 1 = gy`

**数学原理：**
- 加法对每个输入的偏导数都是 1，因此梯度直接传递
- 当发生广播时，被广播的输入在多个位置被使用，其梯度需要累加所有使用位置的梯度
- 具体地，如果 `x1` 被广播到形状 `S`，而原始形状是 `S0`，则 `gx1` 需要在广播维度上求和：`gx1 = sum(gy, dims=广播维度)`

#### 1.1.3 计算示例

##### 示例 1.1.1：相同形状的加法

**前向传播：**
- 输入：`x0 = [[1, 2], [3, 4]]`，形状 `(2, 2)`
- 输入：`x1 = [[5, 6], [7, 8]]`，形状 `(2, 2)`
- 输出：`y = x0 + x1 = [[6, 8], [10, 12]]`，形状 `(2, 2)`

**反向传播：**
- 输出梯度：`gy = [[0.1, 0.2], [0.3, 0.4]]`，形状 `(2, 2)`
- 输入梯度：
  - `gx0 = gy = [[0.1, 0.2], [0.3, 0.4]]`
  - `gx1 = gy = [[0.1, 0.2], [0.3, 0.4]]`

##### 示例 1.1.2：广播加法

**前向传播：**
- 输入：`x0 = [[1, 2], [3, 4]]`，形状 `(2, 2)`
- 输入：`x1 = [10, 20]`，形状 `(2,)`
- 广播后：`x1` 广播为 `[[10, 20], [10, 20]]`，形状 `(2, 2)`
- 输出：`y = [[11, 22], [13, 24]]`，形状 `(2, 2)`

**反向传播：**
- 输出梯度：`gy = [[0.1, 0.2], [0.3, 0.4]]`，形状 `(2, 2)`
- 初始梯度：
  - `gx0 = gy = [[0.1, 0.2], [0.3, 0.4]]`，形状 `(2, 2)` ✓（匹配原始形状）
  - `gx1 = gy = [[0.1, 0.2], [0.3, 0.4]]`，形状 `(2, 2)` ✗（不匹配原始形状 `(2,)`）
- 形状恢复：
  - `gx0` 形状已匹配，无需调整
  - `gx1` 需要压缩：`gx1 = sum_to([[0.1, 0.2], [0.3, 0.4]], (2,)) = [0.1+0.3, 0.2+0.4] = [0.4, 0.6]`
- 最终梯度：
  - `gx0 = [[0.1, 0.2], [0.3, 0.4]]`，形状 `(2, 2)`
  - `gx1 = [0.4, 0.6]`，形状 `(2,)`

**数学解释：**
在广播加法中，`x1` 的每个元素被加到 `x0` 的对应行上。反向传播时，`x1` 的梯度需要累加所有使用该元素的位置的梯度：
- `x1[0] = 10` 被用于 `y[0,0]` 和 `y[1,0]`，所以 `gx1[0] = gy[0,0] + gy[1,0] = 0.1 + 0.3 = 0.4`
- `x1[1] = 20` 被用于 `y[0,1]` 和 `y[1,1]`，所以 `gx1[1] = gy[0,1] + gy[1,1] = 0.2 + 0.4 = 0.6`

### 1.2 减法算子（Sub）

#### 1.2.1 前向传播原理
（待补充）

#### 1.2.2 反向传播原理
（待补充）

#### 1.2.3 计算示例
（待补充）

### 1.3 乘法算子（Mul）

#### 1.3.1 前向传播原理
（待补充）

#### 1.3.2 反向传播原理
（待补充）

#### 1.3.3 计算示例
（待补充）

### 1.4 除法算子（Div）

#### 1.4.1 前向传播原理
（待补充）

#### 1.4.2 反向传播原理
（待补充）

#### 1.4.3 计算示例
（待补充）

### 1.5 矩阵乘法算子（MatMul）

#### 1.5.1 前向传播原理
（待补充）

#### 1.5.2 反向传播原理
（待补充）

#### 1.5.3 计算示例
（待补充）

### 1.6 幂运算算子（Pow）

#### 1.6.1 前向传播原理
（待补充）

#### 1.6.2 反向传播原理
（待补充）

#### 1.6.3 计算示例
（待补充）

### 1.7 指数算子（Exp）

#### 1.7.1 前向传播原理
（待补充）

#### 1.7.2 反向传播原理
（待补充）

#### 1.7.3 计算示例
（待补充）

### 1.8 对数算子（Log）

#### 1.8.1 前向传播原理
（待补充）

#### 1.8.2 反向传播原理
（待补充）

#### 1.8.3 计算示例
（待补充）

### 1.9 取负算子（Neg）

#### 1.9.1 前向传播原理
（待补充）

#### 1.9.2 反向传播原理
（待补充）

#### 1.9.3 计算示例
（待补充）

### 1.10 平方算子（Square）

#### 1.10.1 前向传播原理
（待补充）

#### 1.10.2 反向传播原理
（待补充）

#### 1.10.3 计算示例
（待补充）

### 1.11 求和算子（Sum）

#### 1.11.1 前向传播原理
（待补充）

#### 1.11.2 反向传播原理
（待补充）

#### 1.11.3 计算示例
（待补充）

### 1.12 广播算子（BroadcastTo）

#### 1.12.1 前向传播原理
（待补充）

#### 1.12.2 反向传播原理
（待补充）

#### 1.12.3 计算示例
（待补充）

### 1.13 压缩到指定形状算子（SumTo）

#### 1.13.1 前向传播原理
（待补充）

#### 1.13.2 反向传播原理
（待补充）

#### 1.13.3 计算示例
（待补充）

---

## 二、激活函数算子

### 2.1 ReLU 算子

#### 2.1.1 前向传播原理

ReLU（Rectified Linear Unit）算子的前向传播执行：`y = max(0, x)`

**数学描述：**
- 对于输入 `x` 的每个元素，如果 `x[i] > 0`，则 `y[i] = x[i]`；否则 `y[i] = 0`
- 数学表达式：`y = relu(x) = max(0, x)`
- ReLU 是一个非线性激活函数，用于引入非线性特性

#### 2.1.2 反向传播原理

对于 ReLU 运算 `y = relu(x)`，其导数（梯度）为：
- 当 `x > 0` 时，`∂y/∂x = 1`
- 当 `x ≤ 0` 时，`∂y/∂x = 0`

因此，如果输出梯度为 `gy`，则：
- `gx = gy × ∂y/∂x = gy × mask`，其中 `mask = (x > 0 ? 1 : 0)`

**数学原理：**
- ReLU 的梯度是分段函数：正数位置梯度为 1，非正数位置梯度为 0
- 反向传播时，需要根据前向传播时的输入值 `x` 来判断哪些位置的梯度需要传递
- 具体地，`gx[i] = gy[i]` 如果 `x[i] > 0`，否则 `gx[i] = 0`

#### 2.1.3 计算示例

##### 示例 2.1.1：ReLU 前向与反向传播

**前向传播：**
- 输入：`x = [[-1, 2], [-3, 4]]`，形状 `(2, 2)`
- 输出：`y = relu(x) = [[0, 2], [0, 4]]`，形状 `(2, 2)`
  - `x[0,0] = -1 < 0`，所以 `y[0,0] = 0`
  - `x[0,1] = 2 > 0`，所以 `y[0,1] = 2`
  - `x[1,0] = -3 < 0`，所以 `y[1,0] = 0`
  - `x[1,1] = 4 > 0`，所以 `y[1,1] = 4`

**反向传播：**
- 输出梯度：`gy = [[0.1, 0.2], [0.3, 0.4]]`，形状 `(2, 2)`
- 计算 mask（基于前向输入 `x`）：
  - `mask = [[0, 1], [0, 1]]`（`x > 0` 的位置为 1，否则为 0）
- 输入梯度：`gx = gy × mask`
  ```
  gx = [[0.1, 0.2],    ×    [[0, 1],    =    [[0, 0.2],
        [0.3, 0.4]]          [0, 1]]          [0, 0.4]]
  ```
- 最终梯度：`gx = [[0, 0.2], [0, 0.4]]`，形状 `(2, 2)`

**数学解释：**
- `x[0,0] = -1` 和 `x[1,0] = -3` 在前向传播时输出为 0，因此这些位置的梯度不传播（梯度为 0）
- `x[0,1] = 2` 和 `x[1,1] = 4` 在前向传播时输出等于输入，因此梯度直接传递

### 2.2 Sigmoid 算子

#### 2.2.1 前向传播原理
（待补充）

#### 2.2.2 反向传播原理
（待补充）

#### 2.2.3 计算示例
（待补充）

### 2.3 Softmax 算子

#### 2.3.1 前向传播原理
（待补充）

#### 2.3.2 反向传播原理
（待补充）

#### 2.3.3 计算示例
（待补充）

### 2.4 SiLU 算子

#### 2.4.1 前向传播原理
（待补充）

#### 2.4.2 反向传播原理
（待补充）

#### 2.4.3 计算示例
（待补充）

---

## 三、卷积运算算子

### 3.1 二维卷积算子（Conv2d）

#### 3.1.1 前向传播原理
（待补充）

#### 3.1.2 反向传播原理
（待补充）

#### 3.1.3 计算示例
（待补充）

---

## 四、池化运算算子

### 4.1 最大池化算子（MaxPool2d）

#### 4.1.1 前向传播原理
（待补充）

#### 4.1.2 反向传播原理
（待补充）

#### 4.1.3 计算示例
（待补充）

### 4.2 平均池化算子（AvgPool2d）

#### 4.2.1 前向传播原理
（待补充）

#### 4.2.2 反向传播原理
（待补充）

#### 4.2.3 计算示例
（待补充）

### 4.3 自适应平均池化算子（AdaptiveAvgPool2d）

#### 4.3.1 前向传播原理
（待补充）

#### 4.3.2 反向传播原理
（待补充）

#### 4.3.3 计算示例
（待补充）

---

## 五、形状变换算子

### 5.1 拼接算子（Cat）

#### 5.1.1 前向传播原理

拼接算子将多个张量沿着指定维度拼接在一起。

**数学描述：**
- 输入：`n` 个张量 `[x0, x1, ..., xn]`，每个张量的形状为 `(d0, d1, ..., dk, ..., dm)`
- 拼接维度：`dim = k`，指定在第 `k` 维上进行拼接
- 形状要求：除了第 `k` 维外，所有输入张量的其他维度必须相同
- 输出形状：`(d0, d1, ..., sum(dk_i), ..., dm)`，其中 `sum(dk_i)` 是所有输入在第 `k` 维大小的总和

**数学表示：**
- `y = cat([x0, x1, ..., xn], dim=k)`
- 输出 `y` 在第 `k` 维上包含了所有输入在该维度的数据，按顺序排列

#### 5.1.2 反向传播原理

拼接的反向传播是**分割**：将输出梯度按照原始输入的形状分割回各个输入。

**数学原理：**
- 前向操作：`y = cat([x0, x1, ..., xn], dim=k)`
- 反向操作：`[gx0, gx1, ..., gxn] = split(gy, [shape0, shape1, ..., shapen], dim=k)`
- 核心思想：输出梯度 `gy` 在第 `k` 维上被分割成 `n` 段，每段对应一个输入的梯度
- 分割位置：第 `i` 个输入的梯度 `gxi` 对应 `gy` 在第 `k` 维上的区间 `[sum(dk_0...dk_{i-1}), sum(dk_0...dk_i))`

#### 5.1.3 计算示例

##### 示例 5.1.1：沿第 0 维拼接

**前向传播：**
- 输入：
  - `x0 = [[1, 2], [3, 4]]`，形状 `(2, 2)`
  - `x1 = [[5, 6], [7, 8]]`，形状 `(2, 2)`
  - `x2 = [[9, 10], [11, 12]]`，形状 `(2, 2)`
- 拼接维度：`dim = 0`
- 输出：`y = cat([x0, x1, x2], dim=0)`
  ```
  y = [[1,  2 ],
       [3,  4 ],
       [5,  6 ],
       [7,  8 ],
       [9,  10],
       [11, 12]]
  ```
  形状 `(6, 2)`，其中第 0 维 `6 = 2 + 2 + 2`

**反向传播：**
- 输出梯度：`gy`，形状 `(6, 2)`
- 分割形状：`[(2, 2), (2, 2), (2, 2)]`
- 输入梯度：
  - `gx0 = gy[0:2, :]`，形状 `(2, 2)`
  - `gx1 = gy[2:4, :]`，形状 `(2, 2)`
  - `gx2 = gy[4:6, :]`，形状 `(2, 2)`

##### 示例 5.1.2：沿第 1 维拼接

**前向传播：**
- 输入：
  - `x0 = [[1, 2], [3, 4]]`，形状 `(2, 2)`
  - `x1 = [[5, 6], [7, 8]]`，形状 `(2, 2)`
- 拼接维度：`dim = 1`
- 输出：`y = cat([x0, x1], dim=1)`
  ```
  y = [[1, 2, 5, 6],
       [3, 4, 7, 8]]
  ```
  形状 `(2, 4)`，其中第 1 维 `4 = 2 + 2`

**反向传播：**
- 输出梯度：`gy = [[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]`，形状 `(2, 4)`
- 分割形状：`[(2, 2), (2, 2)]`
- 输入梯度：
  - `gx0 = gy[:, 0:2] = [[0.1, 0.2], [0.5, 0.6]]`，形状 `(2, 2)`
  - `gx1 = gy[:, 2:4] = [[0.3, 0.4], [0.7, 0.8]]`，形状 `(2, 2)`

### 5.2 分割算子（Split）

#### 5.2.1 前向传播原理

分割算子将一个张量沿着指定维度分割成多个张量。

**数学描述：**
- 输入：单个张量 `x`，形状为 `(d0, d1, ..., dk, ..., dm)`
- 分割维度：`dim = k`，指定在第 `k` 维上进行分割
- 分割方式：
  - **固定大小分割**：按固定大小 `s` 分割，第 `k` 维被分成 `⌈dk/s⌉` 段，最后一段可能小于 `s`
  - **大小列表分割**：按指定大小列表 `[s0, s1, ..., sn]` 分割，要求 `sum(si) = dk`
- 输出：`n` 个张量 `[y0, y1, ..., yn]`，每个 `yi` 的形状为 `(d0, d1, ..., si, ..., dm)`

**数学表示：**
- `[y0, y1, ..., yn] = split(x, sizes, dim=k)`
- 每个输出 `yi` 对应输入 `x` 在第 `k` 维上的一个连续区间

#### 5.2.2 反向传播原理

分割的反向传播是**拼接**：将所有分割后的梯度拼接回原始形状。

**数学原理：**
- 前向操作：`[y0, y1, ..., yn] = split(x, sizes, dim=k)`
- 反向操作：`gx = cat([gy0, gy1, ..., gyn], dim=k)`
- 核心思想：分割是拼接的逆操作，因此反向传播就是将分割后的梯度重新拼接
- 梯度连续性：由于前向传播时数据在分割维度上是连续切分的，反向传播时梯度也在同一维度上连续拼接

#### 5.2.3 计算示例

##### 示例 5.2.1：固定大小分割

**前向传播：**
- 输入：`x`，形状 `(10, 3)`
- 分割参数：`split_size = 3`，`dim = 0`
- 计算：
  - `dim_size = 10`
  - `num_splits = (10 + 3 - 1) / 3 = 4`
  - `actual_split_sizes = [3, 3, 3, 1]`
- 输出：
  - `y0 = x[0:3, :]`，形状 `(3, 3)`
  - `y1 = x[3:6, :]`，形状 `(3, 3)`
  - `y2 = x[6:9, :]`，形状 `(3, 3)`
  - `y3 = x[9:10, :]`，形状 `(1, 3)`

**反向传播：**
- 输出梯度：
  - `gy0`，形状 `(3, 3)`
  - `gy1`，形状 `(3, 3)`
  - `gy2`，形状 `(3, 3)`
  - `gy3`，形状 `(1, 3)`
- 输入梯度：`gx = cat([gy0, gy1, gy2, gy3], dim=0)`，形状 `(10, 3)`

##### 示例 5.2.2：大小列表分割

**前向传播：**
- 输入：`x = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]`，形状 `(2, 5)`
- 分割参数：`split_sizes = [2, 3]`，`dim = 1`
- 输出：
  - `y0 = x[:, 0:2] = [[1, 2], [6, 7]]`，形状 `(2, 2)`
  - `y1 = x[:, 2:5] = [[3, 4, 5], [8, 9, 10]]`，形状 `(2, 3)`

**反向传播：**
- 输出梯度：
  - `gy0 = [[0.1, 0.2], [0.3, 0.4]]`，形状 `(2, 2)`
  - `gy1 = [[0.5, 0.6, 0.7], [0.8, 0.9, 1.0]]`，形状 `(2, 3)`
- 输入梯度：`gx = cat([gy0, gy1], dim=1)`
  ```
  gx = [[0.1, 0.2, 0.5, 0.6, 0.7],
        [0.3, 0.4, 0.8, 0.9, 1.0]]
  ```
  形状 `(2, 5)`

### 5.3 重塑算子（Reshape）

#### 5.3.1 前向传播原理
（待补充）

#### 5.3.2 反向传播原理
（待补充）

#### 5.3.3 计算示例
（待补充）

### 5.4 转置算子（Transpose）

#### 5.4.1 前向传播原理
（待补充）

#### 5.4.2 反向传播原理
（待补充）

#### 5.4.3 计算示例
（待补充）

### 5.5 展平算子（Flatten）

#### 5.5.1 前向传播原理
（待补充）

#### 5.5.2 反向传播原理
（待补充）

#### 5.5.3 计算示例
（待补充）

---

## 六、神经网络层算子

### 6.1 Dropout 算子

#### 6.1.1 前向传播原理
（待补充）

#### 6.1.2 反向传播原理
（待补充）

#### 6.1.3 计算示例
（待补充）

### 6.2 上采样算子（Upsample）

#### 6.2.1 前向传播原理

上采样算子将输入张量的空间维度（高度和宽度）进行放大。

**数学描述：**
- 输入：4D 张量 `x`，形状为 `(N, C, H, W)`，其中 `N` 是批次大小，`C` 是通道数，`H` 是高度，`W` 是宽度
- 上采样方式：可以通过指定目标大小 `(H_out, W_out)` 或缩放因子 `(scale_h, scale_w)` 来放大
- 输出形状：`(N, C, H_out, W_out)`，其中 `H_out = H × scale_h`，`W_out = W × scale_w`
- 插值方法：通常使用最近邻插值或双线性插值

**数学表示：**
- `y = upsample(x, size=(H_out, W_out))` 或 `y = upsample(x, scale_factor=(scale_h, scale_w))`
- 输出 `y` 的空间维度被放大，通道数和批次大小保持不变

#### 6.2.2 反向传播原理

上采样的反向传播是**下采样**：将输出梯度按照缩放因子压缩回原始形状。

**数学原理：**
- 前向操作：`y = upsample(x, scale_factor=(scale_h, scale_w))`
- 反向操作：`gx = downsample(gy, scale_factor=(scale_h, scale_w))`
- 核心思想：上采样时，一个输入像素对应多个输出像素；反向传播时，需要将多个输出梯度累加回对应的输入位置
- 梯度累加：对于输入位置 `(i, j)`，其梯度等于所有对应输出位置的梯度之和

**数学表示：**
- 如果 `y[h_out, w_out]` 对应 `x[h, w]`，其中 `h = h_out / scale_h`，`w = w_out / scale_w`
- 则 `gx[h, w] = sum(gy[h_out, w_out])`，对所有满足 `h_out / scale_h = h` 和 `w_out / scale_w = w` 的位置求和

#### 6.2.3 计算示例

##### 示例 6.2.1：上采样前向与反向传播

**前向传播：**
- 输入：`x`，形状 `(1, 1, 2, 2)`（批次大小 1，通道数 1，高度 2，宽度 2）
  ```
  x = [[[[1, 2],
         [3, 4]]]]
  ```
- 缩放因子：`scale_h = 2`，`scale_w = 2`
- 输出：`y`，形状 `(1, 1, 4, 4)`
  ```
  y = [[[[1, 1, 2, 2],
         [1, 1, 2, 2],
         [3, 3, 4, 4],
         [3, 3, 4, 4]]]]
  ```
  （使用最近邻插值，每个输入像素复制到对应的 2×2 区域）

**反向传播：**
- 输出梯度：`gy`，形状 `(1, 1, 4, 4)`
  ```
  gy = [[[[0.1, 0.2, 0.3, 0.4],
          [0.5, 0.6, 0.7, 0.8],
          [0.9, 1.0, 1.1, 1.2],
          [1.3, 1.4, 1.5, 1.6]]]]
  ```
- 输入梯度：`gx`，形状 `(1, 1, 2, 2)`
  - `gx[0, 0]` 对应 `gy[0:2, 0:2]` 的所有元素，所以 `gx[0, 0] = 0.1 + 0.2 + 0.5 + 0.6 = 1.4`
  - `gx[0, 1]` 对应 `gy[0:2, 2:4]` 的所有元素，所以 `gx[0, 1] = 0.3 + 0.4 + 0.7 + 0.8 = 2.2`
  - `gx[1, 0]` 对应 `gy[2:4, 0:2]` 的所有元素，所以 `gx[1, 0] = 0.9 + 1.0 + 1.3 + 1.4 = 4.6`
  - `gx[1, 1]` 对应 `gy[2:4, 2:4]` 的所有元素，所以 `gx[1, 1] = 1.1 + 1.2 + 1.5 + 1.6 = 5.4`
- 最终梯度：
  ```
  gx = [[[[1.4, 2.2],
          [4.6, 5.4]]]]
  ```

**数学解释：**
- 上采样时，`x[0, 0] = 1` 被复制到 `y[0:2, 0:2]` 的 4 个位置
- 反向传播时，这 4 个位置的梯度需要累加回 `gx[0, 0]`
- 类似地，其他输入位置的梯度也是对应输出区域梯度的累加

### 6.3 恒等算子（Identity）

#### 6.3.1 前向传播原理
（待补充）

#### 6.3.2 反向传播原理
（待补充）

#### 6.3.3 计算示例
（待补充）

---

## 七、归一化算子

### 7.1 批归一化算子（BatchNorm）

#### 7.1.1 前向传播原理
（待补充）

#### 7.1.2 反向传播原理
（待补充）

#### 7.1.3 计算示例
（待补充）

---

## 八、损失函数算子

### 8.1 Softmax 交叉熵损失算子（SoftmaxCrossEntropy）

#### 8.1.1 前向传播原理
（待补充）

#### 8.1.2 反向传播原理
（待补充）

#### 8.1.3 计算示例
（待补充）

---

## 九、总结

### 9.1 算子分类总结

| 分类 | 算子 | 特点 |
|------|------|------|
| 数学运算 | Add, Sub, Mul, Div, MatMul, Pow, Exp, Log, Neg, Square, Sum, BroadcastTo, SumTo | 基础数学运算，支持广播 |
| 激活函数 | ReLU, Sigmoid, Softmax, SiLU | 非线性变换，引入非线性特性 |
| 卷积运算 | Conv2d | 特征提取，参数共享 |
| 池化运算 | MaxPool2d, AvgPool2d, AdaptiveAvgPool2d | 降维，减少计算量 |
| 形状变换 | Cat, Split, Reshape, Transpose, Flatten | 改变张量形状和维度 |
| 神经网络层 | Dropout, Upsample, Identity | 网络层操作 |
| 归一化 | BatchNorm | 加速训练，稳定梯度 |
| 损失函数 | SoftmaxCrossEntropy | 计算损失值 |

### 9.2 数学原理总结

**加法算子（Add）：**
- 前向：元素级加法，支持广播
- 反向：基于链式法则，`∂y/∂x0 = 1`，`∂y/∂x1 = 1`，梯度直接传递
- 广播处理：被广播的输入梯度需要在广播维度上求和

**ReLU 算子：**
- 前向：`y = max(0, x)`，非线性激活
- 反向：`gx = gy × mask`，其中 `mask = (x > 0 ? 1 : 0)`
- 特点：正数位置梯度传递，非正数位置梯度为 0

**拼接算子（Cat）：**
- 前向：沿指定维度将多个张量拼接成一个
- 反向：沿同一维度将梯度分割回各个输入，是分割的逆操作

**分割算子（Split）：**
- 前向：沿指定维度将一个张量分割成多个
- 反向：沿同一维度将梯度拼接回原始形状，是拼接的逆操作

**上采样算子（Upsample）：**
- 前向：放大空间维度，一个输入像素对应多个输出像素
- 反向：压缩空间维度，多个输出梯度累加回对应输入位置

### 9.3 对偶关系总结

| 算子对 | 前向操作 | 反向操作 | 关系 |
|--------|---------|---------|------|
| Cat / Split | `y = cat([x0, x1, ...], dim)` | `[gx0, gx1, ...] = split(gy, shapes, dim)` | 互为反向操作 |
| Split / Cat | `[y0, y1, ...] = split(x, sizes, dim)` | `gx = cat([gy0, gy1, ...], dim)` | 互为反向操作 |
| Upsample / Downsample | `y = upsample(x, scale)` | `gx = downsample(gy, scale)` | 互为反向操作 |

**对偶性说明：**
- Cat 和 Split 互为反向操作，体现了算子的对称性
- 这种对偶性使得反向传播的实现更加直观和高效
- 理解对偶关系有助于快速掌握算子的前向和反向传播原理

---

## 参考实现

- `src/operators/math/add.cpp`：加法算子实现
- `src/operators/activation/relu.cpp`：ReLU 算子实现
- `src/operators/shape/cat.cpp`：拼接算子实现
- `src/operators/shape/split.cpp`：分割算子实现
- `src/operators/nn/upsample.cpp`：上采样算子实现
