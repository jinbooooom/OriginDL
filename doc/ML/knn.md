### k近邻
- k近邻法是基本且简单的分类与回归方法。k近邻法的基本做法是：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最近邻的k个实例，这k个实例的多数属于某一个类，就把该输入实例分为这个类。
- k近邻使用的模型实际上对应于对特征空间的一个划分。k近邻法中，当训练集、距离度量、k值及分类决策规则确定后，其结果唯一确定。
- k近邻法三要素：距离度量、k值的选择和分类决策规则。常用的距离度量是欧氏距离及更一般的Lp(可以理解为范数)距离。k值小时，k近邻模型更复杂；k值大时，k近邻模型更简单。k值的选择反映了对近似误差与估计误差之间的权衡，在应用中，k值一般取一个比较小的值(通常是不大于20的整数)，通常由交叉验证选择最优的k[L40]。(交叉验证就是把训练集分成训练集和验证集，首先用训练集对分类器进行训练,再利用验证集来测试训练得到的模型(model),以此来做为评价分类器的性能指标。）
- 常用的分类决策规则是多数表决，对应于经验风险最小化。
- k近邻法的实现需要考虑如何快速搜索k个最近邻点。kd树是一种便于对k维空间中的数据进行快速检索的数据结构。kd树是二叉树，表示对k维空间的一个划分，其每个结点对应于k维空间划分中的一个超矩形区域。利用kd树可以省去对大部分数据点的搜索， 从而减少搜索的计算量。

### k近邻的大致过程（算法思想）
在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，**找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个类别**，其算法的描述为：
```
1. 计算测试数据与各个训练数据之间的距离；
2. 按照距离的递增关系进行排序；
3. 选取距离最小的K个点；
4. 确定前K个点所在类别的出现频率；
5. 返回前K个点中出现频率最高的类别作为测试数据的预测分类。
```

### 推荐/参考博客
- [Python3《机器学习实战》学习笔记（一）：k-近邻算法(史诗级干货长文)](https://zhuanlan.zhihu.com/p/28656126)
- [k近邻与kd树](https://blog.csdn.net/qll125596718/article/details/8426458)　
- [K近邻（KNN）算法、KD树及其python实现](https://blog.csdn.net/sinat_34072381/article/details/84104440)
